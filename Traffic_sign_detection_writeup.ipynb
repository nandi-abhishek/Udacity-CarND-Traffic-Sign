{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Traffic Sign Recognition** \n",
    "\n",
    "**Build a Traffic Sign Recognition Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Load the data set (see below for links to the project data set)\n",
    "* Explore, summarize and visualize the data set\n",
    "* Design, train and test a model architecture\n",
    "* Use the model to make predictions on new images\n",
    "* Analyze the softmax probabilities of the new images\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/visualization.jpg \"Visualization\"\n",
    "[image2]: ./examples/grayscale.jpg \"Grayscaling\"\n",
    "[image3]: ./examples/random_noise.jpg \"Random Noise\"\n",
    "[image4]: ./examples/placeholder.png \"Traffic Sign 1\"\n",
    "[image5]: ./examples/placeholder.png \"Traffic Sign 2\"\n",
    "[image6]: ./examples/placeholder.png \"Traffic Sign 3\"\n",
    "[image7]: ./examples/placeholder.png \"Traffic Sign 4\"\n",
    "[image8]: ./examples/placeholder.png \"Traffic Sign 5\"\n",
    "\n",
    "## Rubric Points\n",
    "### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. You can use this template as a guide for writing the report. The submission includes the project code.\n",
    "\n",
    "In this post, we will go over the work I did for project 2 of Udacityâ€™s self-driving car project, classifying German traffic signs using deep learning. It is very important for a self-driving car to read traffic signs and interpret them to understand the traffic rules. Deep learning neural networks or convolutional neural networks have emerged as powerful image classifiers in the past decade. In this project we will go over the solution for classifying German sign data that gave accuracy of 95.1 on the test data. Here is a link to my [project code](https://github.com/nandi-abhishek/Udacity-CarND-Traffic-Sign/blob/master/Traffic_Sign_Classifier.ipynb)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Summary & Exploration\n",
    "\n",
    "#### 1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n",
    "\n",
    "I used the Numpy library to calculate summary statistics of the German traffic signs data set:\n",
    "\n",
    "The size of training set is 34799.  \n",
    "The size of the validation set is 4410.  \n",
    "The size of test set is 12630.  \n",
    "The shape of a traffic sign image is (32, 32, 3).  \n",
    "The number of unique classes/labels in the data set is 43.  \n",
    "\n",
    "#### 2. Include an exploratory visualization of the dataset.\n",
    "\n",
    "Here is an exploratory visualization of the data set. It is a bar chart showing how the data. It shows that not all classes has large number of samples. Some has around 2000 samples whereas some labels has around 200 examples ...\n",
    "\n",
    "![original_training_set.png](./Writeup_images/original_training_set.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design and Test a Model Architecture\n",
    "\n",
    "#### 1. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc. (OPTIONAL: As described in the \"Stand Out Suggestions\" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, and provide example images of the additional data. Then describe the characteristics of the augmented training set like number of images in the set, number of images for each class, etc.)\n",
    "\n",
    "As a first step, I converted the images to grayscale because grayscaling removes clutter from an image. Since the problem includes only classifying images, grayscaling allows the feature maps to concentrate only on the subject under interest. Also, grayscaling converts a 3-channel RGB image to a single channel image which reduces the computation complexity. Grayscaling was achieved by using the OpenCV's cv2.cvtColor(image, cv2.COLOR_RGB2GRAY).\n",
    "\n",
    "Here is an example of a traffic sign image before and after grayscaling.\n",
    "\n",
    " Original Image  | Gray Scale Image\n",
    "  ------------- | -------------\n",
    " ![Color50.png](./Writeup_images/Color50.png \"Original Image\")   |   ![Gray50.png](./Writeup_images/Gray50.png \"Grayscale Image\")\n",
    "  \n",
    "As the next step, I normalized the image data to the range (-1,1). This was done using the line of code X_train_norm = (X_train - 128.0)/128. This step is necessary because normalization helps in making the neural network converge faster since the variation in data is restricted within a specific range. \n",
    "\n",
    "#### Data Augmentation ####\n",
    "\n",
    "A big limitation of deep neural networks is that they may have millions of parameters, tuning which requires a vast data set. This however is not always possible. In such cases, data augmentation helps us generate additional training examples. I have generated additional data samples by applying affine transformation to the image. Affine transformations refer to transformations that do not alter the parallelism of lines. I have specifically used rotation, shearing and translation to simulate the effect of viewing the sign from different angles and different distances. \n",
    "\n",
    "To generate the augmented data I have taken help from:\n",
    "\n",
    "[Geometric Transformations of Images](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html)\n",
    "\n",
    "Here are some examples of original image and augmented image:\n",
    "\n",
    "Original Image  | Transformed Image\n",
    "  ------------- | -------------\n",
    " ![Original.png](./Writeup_images/Original.png)   |   ![Transformed.png](./Writeup_images/Transformed.png)\n",
    "\n",
    "The total data agmentation step might take 4-5 hrs. The augmented data set and the histogram are given below ...\n",
    "\n",
    "The size of training set is 104599.  \n",
    "The size of the validation set is 4410.  \n",
    "The size of test set is 12630.  \n",
    "The shape of a traffic sign image is (32, 32, 1).  \n",
    "The number of unique classes/labels in the data set is 43.  \n",
    "\n",
    "![augmented_training_set.png](./Writeup_images/augmented_training_set.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.\n",
    "\n",
    "I decided to use a deep neural network classifier as a model. Instead of using the original LeNet-5 architecture my model is adapted from Sermanet/LeCunn traffic sign classification journal article.\n",
    "\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Input         \t\t| 32x32x1 Grayscale Preproceesed Image   \t\t| \n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 28x28x6 \t|\n",
    "| RELU\t\t\t\t\t| Activation Function\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 14x14x6  \t\t\t\t|\n",
    "| Convolution 5x5\t    | 1x1 stride, valid padding, outputs 10x10x16\t|\n",
    "| RELU\t\t\t\t\t| Activation Function\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 5x5x16  \t\t  \t\t    |\n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 1x1x400 \t|\n",
    "| Flattening\t\t    | Flatten 1x1x400 and 5x5x16 layers. Output 800\t|\n",
    "| Dropout       \t\t| Keep Probablity (0.5)\t\t\t\t\t\t\t|\n",
    "| Fully Connected\t\t| Input 800, output 43\t\t\t\t\t\t\t|\n",
    "|\t\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    " \n",
    "![modifiedLeNet.jpeg](./modifiedLeNet.jpeg)\n",
    "\n",
    "#### 3. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n",
    "\n",
    "To train the model, I used the following parameters:  \n",
    "optimizer: Adam  \n",
    "batch size: 100  \n",
    "number of epochs: 50  \n",
    "\n",
    "The other hyperparameters apart from the Kernel size and stride used were:  \n",
    "learning rate: 0.001  \n",
    "mean (for weight initialization): 0  \n",
    "stddev (for weight initialization): 0.1  \n",
    "\n",
    "#### 4. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.\n",
    "\n",
    "My final model results were:  \n",
    "\n",
    "training set accuracy of 99.8%.  \n",
    "validation set accuracy of 96.7%.  \n",
    "test set accuracy of 94.8%.  \n",
    "\n",
    "I chose an iterative approach to modify the Le-Net architecture to get the results:\n",
    "\n",
    "* What was the first architecture that was tried and why was it chosen?  \n",
    "I chose the LeNet-5 architecture as a starting point since it works well on hand-written digits. But, I was not getting good validation set accuracy even with tuning all the hyper parameters. Then I went through Sermanet/LeCunn traffic sign classification journal article and decided to try that\n",
    "\n",
    "* What were some problems with the initial architecture?  \n",
    "There was a lot of overfitting with the initial architecture after feeding the network with the pre-processed data. The training accuracy was about 98% and the validation set accuracy was about 91%.\n",
    "\n",
    "* How was the architecture adjusted and why was it adjusted?  \n",
    "To reduce overfitting, I first tried with L2 regularization. Then I used a dropout layers after before the fully connected layer which gave better result. I experimented with different values for the keep probability and 0.5 seemed to provide the best validation accuracy on my architecture. I have also tried with various amount of pre processed data. Finally, created around 3000 samples per class and got better validation accuracy.\n",
    "\n",
    "* Which parameters were tuned? How were they adjusted and why?  \n",
    "Learning Rate: Tried multiple learning rate 0.001, 0.0005, 0.0001. This was because I saw with increased number of epochs the train and validation accuracy were oscillating. So, I was trying with lower learning rate. But, finally didn't see much difference and chose 0.001.\n",
    "\n",
    "    Dropout: I tried with some dropout percentages and saw 0.5 was reasonable.\n",
    "\n",
    "    Epochs: I increased the number of epochs because the images were not as simple as handwritten digits with 10 classes. The traffic sign dataset contained 43 classes and the complexity of the images was also higher. To encode the information in this training set into the CNN required more nuber of epochs of training. \n",
    "    \n",
    "    Batchsize: Tried multiple batch size like 100, 128, 256, 512 keeping other parameters same. I have observed better result with batch 100.\n",
    "\n",
    "* What are some of the important design choices and why were they chosen? For example, why might a convolution layer work well with this problem? How might a dropout layer help with creating a successful model?  \n",
    "    Convolution layers are useful if the data contains regional similarity i.e. the arrangement of data points gives us useful insight about the spatial information contained in the daata. Images particularly contain useful spatial information which is an important feature. Convolution layers help in extracting these features.\n",
    "\n",
    "    Dropout turns off or sets the activation in some of the neurons in a layer to 0 randomly. This helps the network learn only the most important features in the images ignoring very minute details which might be noise and may change from image to image even belonging to the same class. This prevents the network from overfitting to the training data. It increases the accuracy on the validation set and hence the testing set. So, the network performs better on new images.\n",
    "\n",
    "    Augmenting the database with modified images helped a lot. Initially the netwrok was overfitting a lot. Dropout helped but more than that large amount of augmented images increased validation accuray and reduced overfitting.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test a Model on New Images\n",
    "\n",
    "#### 1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n",
    "\n",
    "Here are some of the German traffic signs that I found on the web:\n",
    "\n",
    "![30kmph.png](./Downloaded_traffic_sign/30kmph.png)\n",
    "![bumpy_road.png](./Downloaded_traffic_sign/bumpy_road.png)\n",
    "![caution_ahead.png](./Downloaded_traffic_sign/caution_ahead.png)\n",
    "![children_crossing.png](./Downloaded_traffic_sign/children_crossing.png)\n",
    "![no_entry.png](./Downloaded_traffic_sign/no_entry.png)\n",
    "![no_truck_passing.png](./Downloaded_traffic_sign/no_truck_passing.png)\n",
    "![right_turn.png](./Downloaded_traffic_sign/right_turn.png)\n",
    "![road_work.png](./Downloaded_traffic_sign/road_work.png)\n",
    "![slippery_road.png](./Downloaded_traffic_sign/slippery_road.png)\n",
    "![stop.png](./Downloaded_traffic_sign/stop.png)\n",
    "\n",
    "In general the images acquired from the web have a higher resolution than the images in the training dataset. These images have a lot of objects apart from just traffic signs. Since, my model does not work on newly seen images, I manually cropped the images so that it only contained the traffic sign. Furthermore, my model only accepts inputs with an aspect ratio of 1:1 and of size 32x32. Hence, I resized them in order to fit them into my model which lead to a loss of detail in the images. These were a few issues that I encountered during data pre-processing.\n",
    "\n",
    "Most of the images in my opinion are clear but the oone with slippery road sign is really difficult to identify.\n",
    "\n",
    "\n",
    "#### 2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the \"Stand Out Suggestions\" part of the rubric).\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| 30 km/h       \t\t| 30 km/h   \t\t\t\t\t\t\t\t\t| \n",
    "| Bumpy Road\t        | Bumpy Road \t\t                 \t\t\t|\n",
    "| General Caution\t\t| General Caution\t\t\t\t\t\t\t\t|\n",
    "| Children crossing\t\t| Pedestrian  \t\t\t\t \t\t\t\t    |\n",
    "| No Entry  \t\t\t| No Entry         \t\t\t\t\t\t\t    |\n",
    "| No passing for vehicles over 3.5 metric tons  | No passing for vehicles over 3.5 metric tons ||\n",
    "| Turn right ahead  \t\t\t| Turn right ahead         \t\t\t\t\t\t\t    |\n",
    "| Road work \t\t\t| Road work         \t\t\t\t\t\t\t    |\n",
    "| Slippery road  \t\t\t| Slippery road       \t\t\t\t\t\t\t    |\n",
    "| Stop \t\t\t| Stop       \t\t\t\t\t\t\t    |\n",
    "\n",
    "\n",
    "The model was able to correctly guess 9 of these 10 traffic signs, which gives an accuracy of 90% which is almost close to the test accuracy. The one that it missed is 'Children Crossing' and predicted 'Pedestrian'. Those two images have similarites to some extent.\n",
    "\n",
    "#### 3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the \"Stand Out Suggestions\" part of the rubric, visualizations can also be provided such as bar charts)\n",
    "\n",
    "The following two images shows the top probablities for each image detected by the model.\n",
    "\n",
    "![Top5probablities.png](./Writeup_images/Top5probablities.png)\n",
    "![top5softmax%20probablities.png](./Writeup_images/top5softmax%20probablities.png)\n",
    "\n",
    "Except that one wrong identified image the model is performing well. In most cases it is 100% sure about its prediction too.\n",
    "\n",
    "### (Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)\n",
    "#### 1. Discuss the visual output of your trained network's feature maps. What characteristics did the neural network use to make classifications?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
